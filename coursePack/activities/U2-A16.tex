\def\theTopic{Errors }
\def\dayNum{16 }

\section{ On Being Wrong 5\% of the Time}


Our confidence in a 95\% confidence interval comes from the fact that,
in the long run, the technique works 95\% of the time to capture the
unknown parameter.  This leads to an old cheap joke:

{\sf Statisticians are people who require themselves to be wrong 5\%
  of the time.}

We hope that's not really true, but decision making leads to a dilemma:\\
  If you want to never be wrong, you have to always put off decisions
  and collect more data.

Statistics allows us to make decisions based on partial data  while
controlling our error rates.\\
Discuss these situations and decide which error would be worse:\vspace{-.6cm}
\begin{enumerate}
\item A criminal jury will make an error if
      they let a guilty defendant go free, or if
      they convict an innocent defendant.
     Which is worse? Why?
\begin{students}
  \vspace{1.5cm}
\end{students}

\item The doctor gives patients a test designed to detect pancreatic
  cancer (which is usually quite serious).  The test is wrong if:
  it says a healthy patient has cancer (a false positive), or if
  it says a patient with cancer is healthy (a false negative).  Which
  is worse?  Why? 
\begin{students}
  \vspace{1.5cm}
\end{students}

\item  An FAA weather forecaster has to decide if it is too dangerous
   (due to storms or potential icing conditions) to allow planes to
   take off.  What is the cost of grounding a flight? What could
   happen if a flight should have been kept on the ground, but wasn't?  
\begin{students}
  \vspace{2.5cm}
\end{students}



\item  Large chain stores are always looking for locations into which
  they can expand -- perhaps into Bozeman. 
  When would a  decision to open a store in Bozeman  be wrong?\\
  When would a decision to {\bf not} open a store in Bozeman  be
  wrong?\\
  What are the trade-offs?  
\begin{students}
  \vspace{3cm}
\end{students}

\end{enumerate}

\subsection{ Two Types of Error }

Definitions:\vspace{-.6cm}
\begin{itemize}
  \item To reject $H_0$ when it is true is called a Type I error.
  \item To fail to reject $H_0$ when it is false is called a Type II error.
\vspace{-.6cm}
\end{itemize}
To remember which is which: we start a hypothesis test by assuming
$H_0$ is true, so Type I goes with $H_0$ being true. 

This table also helps us stay organized: \hfill
\begin{tabular}{|l|c|c|}\hline
   & \multicolumn{2}{|c|}{Decision:} \\
$H_0$ is:  & Reject $H_0$ & Do not reject $H_0$\\\hline
true & {\em Type I Error} & Correct  \\ \hline
false& Correct & {\em Type II error}   \\ \hline\hline
\end{tabular}



{\bf Which is worse?}

The setup for hypothesis testing assumes that we really need to
control the rate of Type I error.  We can do this by setting our
 significance level, $\alpha$.  If, for example, $\alpha = 0.01$, then
 when we reject $H_0$ we are making an error less than 1\% of the time.
So $\alpha$ is the probability of making an error when $H_0$ is true.

There is also a symbol for the probability of a Type II error,
$\beta$, but it changes depending on which alternative parameter
value is correct. Instead of focusing on the negative (error), we
more often talk about the {\bf power} of a test to detect an effect
which is really present.  Power = $1-\beta$ is the probability of
rejecting $H_0$ when it is false (that's a good thing, so we want
power to be high). 



\begin{center}
  {\bf Justice System and Errors }
\end{center}

In both the justice system and in statistics, we can make errors. In
statistics the only way to avoid making errors is to not state any
conclusion without measuring or polling the entire population.  That's
expensive and time consuming, so we instead try to control the chances
of making an error.  
 

For a scientist, committing a Type I error means we could report a
big discovery when in fact, nothing is going on. (How embarrassing!)
This is considered to be more critical than a Type II error, which happens if
the scientist does a research project and finds no ``effect'' when, in
fact, there is one. Type I error rate is controlled by setting
$\alpha$, the significance level, and only rejecting $H_0$ when the
p-value is less than $\alpha$. 


Type II error is harder to control because it depends on these things:
\vspace{-.2cm}
\begin{itemize}
\item  Sample size.  P--values are strongly affected by sample
  size. With a big sample we can detect small differences.  With small
  samples, only coarse or obvious ones.
\item  Significance level.  The fence, or $\alpha$ (alpha), is
  usually set at .10, .05 or .01 with smaller values requiring
  stronger evidence before we reject the null hypothesis and thus
  lower probability of error.  
\item  The null hypothesis has to be wrong, but it could be wrong just
  by a small amount or by a large amount.  For example if 
   we did not reject the null hypothesis that treatment and
  control  were equally effective,  we could be making a type II
  error.  If in fact,  there was a small difference, it would be
  hard to detect, and if the treatment was far better, it would
  be easy to detect.  This is called the effect size, which is
  [difference between null model mean and an alternative mean] divided
  by standard error.
\end{itemize}


The Power Demo web app lets us try different values to see what size
power we get.  Go to 
\webAppURLFrst\  and click
\fbox{Power Demo} under \fbox{One Quant}.

\begin{center}
  {\bf Try different sample sizes.}
\end{center}
\begin{enumerate}
  \setcounter{enumi}{4}
  \item  Set SD to 2, Alternative Mean to 1.5, and
    Significance Level to 0.01. Find the power and the effect size for
    each sample size:\\ 
\begin{students}
  \begin{tabular}{ |l|r|r|}\hline
    n & power& effect size\\ \hline
    {\large 4} &&\\ \hline
    {\large 8} &&\\ \hline
    {\large 16} &&\\ \hline
    {\large 32} &&\\ \hline
    {\large 48} &&\\ \hline
  \end{tabular}
\end{students}

\begin{key}
  \begin{tabular}{ |l|r|r|}\hline
    n & power& effect size\\ \hline
     4 &0.085& 0.75\\ \hline
     8 &0.274& 0.75\\ \hline
     16 &0.656& 0.75\\ \hline
     32 &0.958 &0.75\\ \hline
     48 &0.997 &0.75\\ \hline
  \end{tabular}
\end{key}
\item      What happens to power as you increase sample size? Explain why.
\begin{students}
\vspace{2cm} %% 
\end{students}

\begin{key}
  {\it It appears that the distribution under the null and the
    alternative mean get further apart.  In fact, variance decreases
    as sample size increases. }
\end{key}

\begin{center}
  {\bf Try different standard deviations.}
\end{center}

  \item  Set sample size to 16, Alternative Mean to 1.5, and
    Significance Level to 0.01. Find the power and the effect size for
    each Standard Deviation (SD). This SD is the spread of individual
    data points within our sample.\\ 
\begin{students}
  \begin{tabular}{ |l|r|r|}\hline
    SD & power& effect size\\ \hline
    {\large 0.4} && \\ \hline
    {\large 1} &&\\ \hline
    {\large 2} &&\\ \hline
    {\large 3} &&\\ \hline
    {\large 4} &&\\ \hline
  \end{tabular}
\end{students}

\begin{key}
  \begin{tabular}{ |l|r|r|}\hline
     SD & power& effect size\\ \hline
    {\large 0.4}&1.00& 3.75\\ \hline
    {\large 1}  &0.999&1.5\\ \hline
    {\large 1.5}&0.856&0.938\\ \hline
    {\large 2}  &0.656&0.75\\ \hline
    {\large 3}  &0.307&0.5\\ \hline
  \end{tabular}
\end{key}
\item      What happens to power as you increase standard deviation? Explain why.
\begin{students}
\vspace{2cm} %% 
\end{students}

\begin{key}
  {\it It appears that the distribution under the null and the
    alternative mean get closer together.  In fact, variance increases
    as SD increases. }
\end{key}


\begin{center}
  {\bf Try different alternative means.}
\end{center}

  \item  Set sample size to 16, SD to 2, and
    Significance Level to 0.01. Find the power and the effect size for
    each Alternative Mean.\\ 
\begin{students}
  \begin{tabular}{ |l|r|r|}\hline
    Alternative Mean & power& effect size\\ \hline
    {\large 0.4} && \\ \hline
    {\large 0.8} &&\\ \hline
    {\large 1.2} &&\\ \hline
    {\large 1.6} &&\\ \hline
    {\large 2.4} &&\\ \hline
  \end{tabular}
\end{students}

\begin{key}
  \begin{tabular}{ |l|r|r|}\hline
     Alt. Mean & power& effect size\\ \hline
    {\large 0.4} &0.079& 0.25\\ \hline
    {\large 0.8} &0.307&0.5\\ \hline
    {\large 1.2}&0.656 &0.75\\ \hline
    {\large 1.6}&0.903 &1.00\\ \hline
    {\large 2.4}&0.999 &1.5\\ \hline
  \end{tabular}
\end{key}
\item      What happens to power as you increase the alternative mean? Explain why.
\begin{students}
\vspace{2cm} %% 
\end{students}

\begin{key}
  {\it As the alternative mean increases, it gets easier to
    distinguish the two means, and hence easier to reject $H_0$ which
    implies greater power.  }
\end{key}

\item Look at the effect sizes in the last two tables.
    How do SD and Alternative Mean work together to determine power? 
\begin{students}
 \vspace{2cm}
\end{students}

\begin{key}
  {\it     Larger effect for same SD means more power, lower SD for same
    effect means more power.  In general, larger effect and lower
    SD means more power. A given effect size has the same power (for
    fixed $n$) no matter what SD and Alternative Mean we have.}
\end{key}


\begin{center}
  {\bf Try different significance levels.}
\end{center}

  \item  Set sample size to 16, SD to 3, and
    Alternative mean to 1. Find the power and the effect size for
    each significance level ($\alpha$).\\ 
\begin{students}\hline
  \begin{tabular}{ |l|r|r|}\hline
    $\alpha$ & power& effect size\\ \hline
    {\large 0.01} && \\ \hline
    {\large 0.03} &&\\ \hline
    {\large 0.05} &&\\ \hline
    {\large 0.07} &&\\ \hline
    {\large 0.10} &&\\ \hline
  \end{tabular}
\end{students}

\begin{key}
  \begin{tabular}{ |l|r|r|}\hline
    $\alpha$ & power& effect size\\ \hline
    {\large 0.01}&0.54 & 0.667\\ \hline
    {\large 0.03}&0.734 &0.667\\ \hline
    {\large 0.05}&0.816 &0.667\\ \hline
    {\large 0.07}&0.862 &0.667\\ \hline
    {\large 0.10}&0.905 &0.667\\ \hline
  \end{tabular}
\end{key}

  \item    In which direction does power change when we increase the
    significance level?  
\begin{students}
 \vspace{3cm}
\end{students}

\begin{key}
  {\it     It would increase (power and significance level change in the
    same direction). }
\end{key}

\begin{center}
  {\bf Planning a new study}
\end{center}
  \item   Suppose that we are planning to do a study of how energy
    drinks effect RBAN scores similar to the study we read about in
    Activity 14.  From previous data, we have an estimate of standard
    deviation of 3.8. We plan to use a significance level of $\alpha =
    .05$, and want to be able to detect an increase in mean RBAN score
    of 2 with 90\% power.   How large must our sample size be?
\begin{students}
 \vspace{1cm} %%
\end{students}

\begin{key}
  {\it  33}
\end{key}


      If we choose $\alpha = .01$, how large a sample is needed?
\begin{students}
 \vspace{1cm}
\end{students}
\begin{key}
\ \ \   {\it 50}
\end{key}

  \item  Now suppose that we are using a memory test used to study
    sleep deprivation.  Historical data 
    provides an estimate of SD = 13.  We want to use $\alpha = .05$ and
    need to detect a decrease in mean score (when people are sleep
    deprived) of 6 with 80\% power.  
    How large a sample is needed? 
\begin{students}
 \vspace{1cm} %%
\end{students}

\begin{key}
  {\it  31}
\end{key}


    If we want to limit the chance of Type II error to 10\% or less,
    how large a sample size is needed? 
\begin{students}
 \vspace{1cm}\\
\end{students}

\begin{key}
  {\it  Type II = 1--power so we want more than 90\% power.  Need 43
      people. }
\end{key}
\item Suppose we do another study on energy drinks with alcohol using
  Control and REDA.  This time we test hand-eye coordination using
  $H_0:\ \mu_{control} = \mu_{REDA}$ versus alternative  $H_a:\
  \mu_{control} > \mu_{REDA}$.
  \begin{enumerate}
  \item What would be a Type I error in this context? 
\begin{students}
 \vspace{3cm}\\
\end{students}

\begin{key}
  {\it   To conclude that there is a difference in mean coordination between the
    two treatments when, in fact, REDA has no effect on coordination scores. }
\end{key}
  \item What would be a Type II error in this context? 
\begin{students}
 \vspace{5cm}\\
\end{students}

\begin{key}
  {\it  To fail to find a difference in mean coordination between the
    two treatments when, in fact, REDA lowers coordination scores. }
\end{key}
  \end{enumerate}

\end{enumerate}


\begin{center}
  {\bf Take Home Message} \vspace{-.6cm}
\end{center}
\begin{itemize}
  \item Errors happen.  Use of statistics does not prevent all errors,
    but it does limit them to a level we can tolerate. We have labels
    for two types of error.
  \item The talk about probability of  error is based on the sampling
    distribution assuming random assignment of treatments or random
    sampling. It's really a ``best case'' scenario, because there
    could be other sources of error we have not considered.  For
    example, we could have not sampled from some part of the
    population, or we could have errors in our measuring tools.
  \item If you are designing a study, you might need to consult a
    statistician to help determine how large a sample size is
    needed. You'll need to decide what $\alpha$ to use, what the
    underlying variation is ($\sigma$), and how large a difference you
    need to detect with a certain level of power.
 \item 
  Questions? Summarize the  lesson. 
\end{itemize}\vfill



\noindent
{\bf Assignment} \vspace{-.2in}
\begin{itemize}
% \item  D2Quiz 7 is due March 7. 
% \item {\bf D2Box 7} is due March 10.
%     Turn it in as a pdf file in the Drop Box on D2L.
 %%  We strongly encourage you to get help in the Math Learning Center.
\item Read the next two pages before your next class.
\item Watch the ``Correlation and Regression'' video (\# 5 under Unit 2).
\end{itemize}
